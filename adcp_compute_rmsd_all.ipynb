{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RMSD ADCP â€“ Verger-Miralles et al. (2025)\n",
    "\n",
    "**SWOT enhances small-scale eddy detection in the Mediterranean Sea**\n",
    "\n",
    "Author: *Elisabet Verger-Miralles*  \n",
    "Institution: IMEDEA (CSIC-UIB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute RMSD ADCP cross-section vels. vs SWOT and DUACS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob \n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import cmath\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import bootstrap\n",
    "from scipy.interpolate import interp1d, griddata\n",
    "from functions import haversine, low_pass_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_adcp = '../../grl_codes_to_publish_def_swotv2.0.1/data/ADCP/'\n",
    "dir_SWOT = '../../grl_codes_to_publish_def_swotv2.0.1/data/SWOT/'\n",
    "dir_duacs = '../../SWOT/imatges/SSH/reprocessed/'\n",
    "file_swot = 'SWOT_L3_LR_SSH_Expert_502_016_20230426T062612_20230426T071716_v2.0.1.nc'  \n",
    "file_duacs = 'cmems_obs-sl_eur_phy-ssh_my_allsat-l4-duacs-0.125deg_P1D_20230426.nc'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data, compute cross-section velocities and concatenate the differences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transects to process\n",
    "transects = [6, 8, 9, 10, 11, 12, 13]\n",
    "depth_level = 80 \n",
    "\n",
    "# Load datasets\n",
    "ds_adcp = xr.open_dataset(dir_adcp + 'adcp_selected_transects_fast_swot.nc')\n",
    "df_adcp = ds_adcp.to_dataframe().reset_index(drop=False)\n",
    "ds_swot = xr.open_dataset(dir_SWOT + file_swot)\n",
    "ds_duacs = xr.open_dataset(dir_duacs + file_duacs)\n",
    "\n",
    "# Storage for results\n",
    "rmsd_s_all = []\n",
    "rmsd_d_all = []\n",
    "uv_ADCP_cross_all = []\n",
    "uv_DUACS_cross_all = []\n",
    "uv_SWOT_cross_all = []\n",
    "\n",
    "for trans in transects:\n",
    "    # ADCP data\n",
    "    lon_adcp = np.array(df_adcp.loc[(df_adcp.transect_id == trans) & (df_adcp.depth == depth_level), 'lon'])[1:]\n",
    "    lat_adcp = np.array(df_adcp.loc[(df_adcp.transect_id == trans) & (df_adcp.depth == depth_level), 'lat'])[1:]\n",
    "    u_adcp = df_adcp.loc[(df_adcp.transect_id == trans) & (df_adcp.depth == depth_level), 'u'].values[1:]\n",
    "    v_adcp = df_adcp.loc[(df_adcp.transect_id == trans) & (df_adcp.depth == depth_level), 'v'].values[1:]\n",
    "\n",
    "    # Distance calculation\n",
    "    lat0, lon0 = lat_adcp[0], lon_adcp[0]\n",
    "    distances = np.array([haversine(lat0, lon0, lat, lon) for lat, lon in zip(lat_adcp, lon_adcp)])\n",
    "    regular_distances = np.arange(0, distances[-1], 0.25)\n",
    "\n",
    "    # Interpolation\n",
    "    interpolator = interp1d(distances, list(zip(lat_adcp, lon_adcp)), axis=0, kind='linear')\n",
    "    interpolated_lats, interpolated_lons = interpolator(regular_distances).T\n",
    "\n",
    "    u_interp = interp1d(distances, u_adcp)(regular_distances)\n",
    "    v_interp = interp1d(distances, v_adcp)(regular_distances)\n",
    "\n",
    "    # Filtering\n",
    "    fs = 1 / (regular_distances[1] - regular_distances[0])\n",
    "    cutoff_distance = 0.067\n",
    "    u_filt = low_pass_filter(u_interp, cutoff_distance, fs)\n",
    "    v_filt = low_pass_filter(v_interp, cutoff_distance, fs)\n",
    "\n",
    "    # Transect angle\n",
    "    if trans == 6:\n",
    "        lon1, lat1 = 1.41, 39.65\n",
    "        lon2, lat2 = 1.58, 40\n",
    "        dlon = lon1 - lon2\n",
    "        dlat = lat1 - lat2\n",
    "        \n",
    "    else:\n",
    "        lon1, lat1 = 1.73, 39.8\n",
    "        lon2, lat2 = 1.3, 39.9\n",
    "        dlon=lon2-lon1;\n",
    "        dlat=lat2-lat1;\n",
    "    \n",
    "\n",
    "    mean_lat = np.radians((lat1 + lat2) / 2.0)\n",
    "    angle = np.degrees(np.arctan2(dlat, dlon * np.cos(mean_lat)))\n",
    "\n",
    "    # Rotate velocities\n",
    "    uv_cross = ((u_filt + 1j * v_filt) * cmath.exp(-1j * np.radians(angle))).imag\n",
    "\n",
    "    # SWOT interpolation\n",
    "    lon_min, lon_max = lon_adcp.min()-0.5, lon_adcp.max()+0.5\n",
    "    lat_min, lat_max = lat_adcp.min()-0.5, lat_adcp.max()+0.5\n",
    "\n",
    "    swot = ds_swot.where(\n",
    "    (ds_swot.longitude >= lon_min) & (ds_swot.longitude <= lon_max) &\n",
    "    (ds_swot.latitude >= lat_min) & (ds_swot.latitude <= lat_max),\n",
    "    drop=True)\n",
    "\n",
    "    u_swot = swot.ugos_filtered.values.flatten()\n",
    "    v_swot = swot.vgos_filtered.values.flatten()\n",
    "    lon_swot = swot.longitude.values.flatten()\n",
    "    lat_swot = swot.latitude.values.flatten()\n",
    "\n",
    "    swot_mask = ~np.isnan(u_swot)\n",
    "    u_swot = u_swot[swot_mask]\n",
    "    v_swot = v_swot[swot_mask]\n",
    "    lon_swot = lon_swot[swot_mask]\n",
    "    lat_swot = lat_swot[swot_mask]\n",
    "\n",
    "    u_swot_i = griddata((lon_swot, lat_swot), u_swot, (interpolated_lons, interpolated_lats), method='cubic')\n",
    "    v_swot_i = griddata((lon_swot, lat_swot), v_swot, (interpolated_lons, interpolated_lats), method='cubic')\n",
    "    uv_SWOT_cross = ((u_swot_i + 1j * v_swot_i) * cmath.exp(-1j * np.radians(angle))).imag\n",
    "\n",
    "    # DUACS interpolation\n",
    "    lon_duacs, lat_duacs = np.meshgrid(ds_duacs.longitude, ds_duacs.latitude)\n",
    "    u_duacs = np.array(ds_duacs.ugos).ravel()\n",
    "    v_duacs = np.array(ds_duacs.vgos).ravel()\n",
    "\n",
    "    # convert u_duacs and v_duacs nan values to zero\n",
    "    u_duacs = np.nan_to_num(u_duacs, nan=0.0)\n",
    "    v_duacs = np.nan_to_num(v_duacs, nan=0.0)\n",
    "    \n",
    "    lon_d = lon_duacs.ravel()\n",
    "    lat_d = lat_duacs.ravel()\n",
    "\n",
    "    u_duacs_i = griddata((lon_d, lat_d), u_duacs, (interpolated_lons, interpolated_lats), method='cubic')\n",
    "    v_duacs_i = griddata((lon_d, lat_d), v_duacs, (interpolated_lons, interpolated_lats), method='cubic')\n",
    "    uv_DUACS_cross = ((u_duacs_i + 1j * v_duacs_i) * cmath.exp(-1j * np.radians(angle))).imag\n",
    "\n",
    "    # RMSD\n",
    "    rmsd_s = (uv_cross - uv_SWOT_cross)**2\n",
    "    rmsd_d = (uv_cross - uv_DUACS_cross)**2\n",
    "\n",
    "    rmsd_s_all.append(rmsd_s)\n",
    "    rmsd_d_all.append(rmsd_d)\n",
    "\n",
    "    uv_ADCP_cross_all.append(uv_cross)\n",
    "    uv_DUACS_cross_all.append(uv_DUACS_cross)\n",
    "    uv_SWOT_cross_all.append(uv_SWOT_cross)\n",
    "\n",
    "\n",
    "# Concatenate all transects\n",
    "rmsd_s_all = np.concatenate(rmsd_s_all)\n",
    "rmsd_d_all = np.concatenate(rmsd_d_all)\n",
    "uv_ADCP_cross_all = np.concatenate(uv_ADCP_cross_all)\n",
    "uv_DUACS_cross_all = np.concatenate(uv_DUACS_cross_all)\n",
    "uv_SWOT_cross_all = np.concatenate(uv_SWOT_cross_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the concatenated arrays\n",
    "mean_rmsd_s = np.sqrt(np.nanmean(rmsd_s_all))\n",
    "mean_rmsd_d = np.sqrt(np.nanmean(rmsd_d_all))\n",
    "\n",
    "# Calculate the percentage improvement\n",
    "percent_improvement = 100 * (mean_rmsd_d - mean_rmsd_s) / mean_rmsd_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(8.91665848093341),\n",
       " np.float64(13.814783930644248),\n",
       " np.float64(35.455679034152034))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_rmsd_s * 100 , mean_rmsd_d * 100, percent_improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOOTSTRAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DUACS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSD: 0.1388\n",
      "95% Confidence Interval: ConfidenceInterval(low=np.float64(0.13601159921957126), high=np.float64(0.14227916117771314))\n"
     ]
    }
   ],
   "source": [
    "# Define RMSD function that takes two separate 1D arrays\n",
    "def rmsd(drifter, swot, axis=0):\n",
    "    \"\"\"Compute Root Mean Square Deviation (RMSD).\"\"\"\n",
    "    diff = drifter - swot\n",
    "    return np.sqrt(np.nanmean(diff**2, axis=axis))\n",
    "\n",
    "# Combine data into a tuple without reshaping\n",
    "data = (uv_ADCP_cross_all, uv_DUACS_cross_all)\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "res = bootstrap(\n",
    "    data, \n",
    "    statistic=rmsd, \n",
    "    n_resamples=1000, \n",
    "    confidence_level=0.95, \n",
    "    method='BCa',  # Bias-Corrected and Accelerated bootstrap method\n",
    "    paired=True,  # Since we compare paired velocity values\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"RMSD: {rmsd(uv_ADCP_cross_all, uv_DUACS_cross_all):.4f}\")\n",
    "\n",
    "print(f\"95% Confidence Interval: {res.confidence_interval}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.13601159921957126), np.float64(0.14227916117771314))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low confidence interval\n",
    "low_ci = res.confidence_interval[0]\n",
    "high_ci = res.confidence_interval[1]\n",
    "low_ci, high_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.002788400780428746), np.float64(0.0034791611777131304))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.1388 - low_ci, high_ci - 0.1388 # longitude of the error bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3133780979070938)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((0.1388 - low_ci)*100) + ((high_ci - 0.1388)*100))/2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SWOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSD: 0.0888\n",
      "95% Confidence Interval: ConfidenceInterval(low=np.float64(0.08517299453592632), high=np.float64(0.09241633280744484))\n"
     ]
    }
   ],
   "source": [
    "data = (uv_ADCP_cross_all, uv_SWOT_cross_all)\n",
    "\n",
    "# Perform bootstrap resampling\n",
    "res = bootstrap(\n",
    "    data, \n",
    "    statistic=rmsd, \n",
    "    n_resamples=1000, \n",
    "    confidence_level=0.95, \n",
    "    method='BCa',  # Bias-Corrected and Accelerated bootstrap method\n",
    "    paired=True,  # Since we compare paired velocity values\n",
    "    random_state=42  # For reproducibility\n",
    ")\n",
    "\n",
    "# Print results\n",
    "print(f\"RMSD: {rmsd(uv_ADCP_cross_all, uv_SWOT_cross_all):.4f}\")\n",
    "\n",
    "print(f\"95% Confidence Interval: {res.confidence_interval}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.08517299453592632), np.float64(0.09241633280744484))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low confidence interval\n",
    "low_ci = res.confidence_interval[0]\n",
    "high_ci = res.confidence_interval[1]\n",
    "low_ci, high_ci"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(0.003627005464073685), np.float64(0.003616332807444833))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.0888 - low_ci, high_ci - 0.0888 # longitude of the error bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.3621669135759259)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(((0.0888 - low_ci)*100) + ((high_ci - 0.0888)*100))/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36.023054755043226"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "percent_improvement = 100 * (0.1388 - 0.0888) / 0.1388\n",
    "percent_improvement"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_elisabet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
